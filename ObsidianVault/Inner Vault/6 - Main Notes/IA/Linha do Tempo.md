
A linha temporal da IA Ã© mais antiga do que se pensa:

![[ai-timeline.png]]

A partir disso, temos uma segunda linha temporal que faz uma espÃ©cie de _overlap_

![[ai-timeline2.png]]

## ğŸ§  2012 â€” **AlexNet**

**Ponto de virada do Deep Learning**

- A AlexNet vence a competiÃ§Ã£o ImageNet com larga vantagem.
    
- Mostra que **redes neurais profundas + GPUs** funcionam muito melhor que tÃ©cnicas clÃ¡ssicas.
    
- Resultado:  
    ğŸ‘‰ **VisÃ£o computacional migra de vez para redes neurais**
    

ğŸ“Œ Aqui comeÃ§a a â€œera modernaâ€ do Deep Learning.

---

## ğŸ‘ï¸ 2013â€“2014 â€” **CNNs, VAEs e GANs**

- **CNNs** se consolidam em visÃ£o computacional.
    
- **VAEs (Variational Autoencoders)** introduzem geraÃ§Ã£o probabilÃ­stica.
    
- **GANs (2014)** trazem geraÃ§Ã£o realista de imagens:
    
    - Um modelo gera
        
    - Outro critica (discriminador)
        

ğŸ¨ Primeiro grande salto em **modelos generativos visuais**.

---

## ğŸ” 2015 â€” **ResNets, RNNs e LSTMs**

- **ResNets** resolvem o problema de treinar redes muito profundas.
    
- **RNNs e LSTMs** dominam:
    
    - TraduÃ§Ã£o automÃ¡tica
        
    - Texto
        
    - SÃ©ries temporais
        

ğŸ“Œ Ainda Ã© a era â€œsequencialâ€ do NLP (antes dos transformers).

---

## â™Ÿï¸ 2016 â€” **AlphaGo**

- IA vence o campeÃ£o mundial de Go.
    
- CombinaÃ§Ã£o de:
    
    - Deep Learning
        
    - Reinforcement Learning
        
    - Busca em Ã¡rvore
        

ğŸ’¥ Mostra que redes neurais podem **raciocinar estrategicamente**, nÃ£o sÃ³ reconhecer padrÃµes.

---

## ğŸ”„ 2017 â€” **Transformers**

**O evento mais importante da linha do tempo**

- Paper: _â€œAttention Is All You Needâ€_
    
- Substitui RNNs por **atenÃ§Ã£o**
    
- Vantagens:
    
    - Paralelismo
        
    - Contexto global
        
    - Escala massiva
        

ğŸ“Œ Tudo que vem depois (GPT, BERT, ChatGPT) depende disso.

---

## ğŸ§  2018 â€” **GPT-1, BERT e Graph Neural Networks**

- **BERT**: compreensÃ£o de texto bidirecional.
    
- **GPT-1**: texto gerado de forma autoregressiva.
    
- **GNNs**: aprendizado em grafos (redes, molÃ©culas, relaÃ§Ãµes).
    

ğŸ§© NLP passa a realmente â€œentenderâ€ contexto.

---

## âœï¸ 2019 â€” **GPT-2**

- Texto coerente em longos trechos.
    
- GeraÃ§Ã£o comeÃ§a a parecer â€œhumanaâ€.
    

âš ï¸ Primeiras discussÃµes sÃ©rias sobre riscos e impacto social.

---

## ğŸ§‘â€ğŸ« 2020 â€” **GPT-3 & Self-Supervised Learning**

- 175 bilhÃµes de parÃ¢metros.
    
- Aprende a partir de grandes volumes de texto **sem rÃ³tulos manuais**.
    
- Surge o conceito forte de **modelos fundacionais**.
    

ğŸ“Œ â€œUm modelo para muitas tarefasâ€.

---

## ğŸ§¬ 2021 â€” **AlphaFold 2, DALLÂ·E e Copilot**

- **AlphaFold 2** resolve o problema de dobramento de proteÃ­nas.
    
- **DALLÂ·E** gera imagens a partir de texto.
    
- **GitHub Copilot** leva IA direto ao fluxo do dev.
    

ğŸš€ IA sai do laboratÃ³rio e entra no dia a dia.

---

## ğŸ’¬ 2022 â€” **ChatGPT & Stable Diffusion**

- Interfaces conversacionais explodem.
    
- **Stable Diffusion** democratiza geraÃ§Ã£o de imagens.
    
- IA se torna:
    
    - AcessÃ­vel
        
    - Interativa
        
    - Popular
        

ğŸ“Œ Aqui comeÃ§a a adoÃ§Ã£o em massa.

---

## ğŸŒ± 2023 â€” **IA Generativa**

- ConsolidaÃ§Ã£o do conceito:
    
    - Texto
        
    - Imagem
        
    - CÃ³digo
        
    - Ãudio
        
    - VÃ­deo
        
- IA deixa de ser sÃ³ analÃ­tica â†’ passa a ser **criativa**.


Para completar, essas Ãºltimas linha do tempo mostram que as inovaÃ§Ãµes nÃ£o estÃ£o mais demorando anos mas sim semanas.

## 2023
![[Screenshot 2026-01-06 at 20.03.07.png]]

## 2024
![[Screenshot 2026-01-06 at 20.04.14.png]]

## 2025
![[Screenshot 2026-01-06 at 20.04.52.png]]

Fora outros modelos que foram lanÃ§ados como Perplexity, Kwen, GPT 5, etc.